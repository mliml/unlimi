# OpenAI Prompt æ—¥å¿—ç³»ç»Ÿ

## ğŸ“‹ æ¦‚è¿°

æœ¬ç³»ç»Ÿå®ç°äº†å¯¹ OpenAI API è°ƒç”¨çš„å®Œæ•´ prompt è®°å½•åŠŸèƒ½ï¼Œ**ä»…å¯¹ `is_admin=true` çš„ç”¨æˆ·ç”Ÿæ•ˆ**ã€‚

### ä¸»è¦ç‰¹æ€§

âœ… è®°å½•å®Œæ•´çš„ messagesï¼ˆsystem + user + historyï¼‰
âœ… è®°å½• token ä½¿ç”¨æƒ…å†µï¼ˆåŒ…æ‹¬ç¼“å­˜ tokensï¼‰
âœ… ä»…å¯¹ admin ç”¨æˆ·è®°å½•ï¼Œä¿æŠ¤æ™®é€šç”¨æˆ·éšç§
âœ… ä½¿ç”¨ contextvars ç¡®ä¿çº¿ç¨‹å®‰å…¨
âœ… JSONL æ ¼å¼ï¼Œæ–¹ä¾¿åç»­åˆ†æ
âœ… æŒ‰å¤©åˆ†å‰²æ—¥å¿—æ–‡ä»¶

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### è°ƒç”¨é“¾

```
API è¯·æ±‚ â†’ therapist_agent_service.chat()
    â†“
è®¾ç½® openai_logging_context (user_id, session_id, is_admin)
    â†“
Agent.run() â†’ OpenAIChat.invoke()
    â†“
HTTP Client (with hooks) â†’ OpenAI API
    â†“
Request Hook: æ‹¦æˆªè¯·æ±‚ï¼Œè®°å½• messages
Response Hook: æ‹¦æˆªå“åº”ï¼Œè®°å½• usage
```

### æ ¸å¿ƒç»„ä»¶

1. **`app/core/openai_logger.py`**
   - `OpenAIPromptLogger`: æ—¥å¿—è®°å½•å™¨
   - `create_logging_http_client()`: åˆ›å»ºå¸¦ hooks çš„ HTTP client
   - `openai_logging_context()`: ä¸Šä¸‹æ–‡ç®¡ç†å™¨

2. **`app/agents/therapist_agent_service.py`**
   - åœ¨ `__init__` ä¸­ä½¿ç”¨è‡ªå®šä¹‰ HTTP client
   - åœ¨ `chat()` ä¸­è®¾ç½®æ—¥å¿—ä¸Šä¸‹æ–‡

3. **`scripts/view_prompts.py`**
   - æ—¥å¿—æŸ¥çœ‹å’Œå¯¼å‡ºå·¥å…·

---

## ğŸ“Š OpenAI Token è®¡è´¹è¯¦è§£

### Token ç±»å‹

```json
{
  "usage": {
    "prompt_tokens": 150,           // æ€»è¾“å…¥ tokens
    "completion_tokens": 50,        // è¾“å‡º tokens
    "total_tokens": 200,            // æ€»è®¡
    "prompt_tokens_details": {
      "cached_tokens": 100,         // ğŸ”¥ ç¼“å­˜çš„ tokensï¼ˆä¾¿å®œ 50%ï¼‰
      "audio_tokens": 0             // éŸ³é¢‘è¾“å…¥
    },
    "completion_tokens_details": {
      "reasoning_tokens": 0,        // o1 æ¨¡å‹çš„æ¨ç† tokens
      "audio_tokens": 0             // éŸ³é¢‘è¾“å‡º
    }
  }
}
```

### ç¼“å­˜æœºåˆ¶ï¼ˆPrompt Cachingï¼‰

OpenAI ä¼šè‡ªåŠ¨ç¼“å­˜ç›¸ä¼¼çš„ prompt å‰ç¼€ï¼š

- **é¦–æ¬¡è¯·æ±‚**: 150 tokensï¼Œå…¨ä»· $0.15/1M
- **åç»­è¯·æ±‚**:
  - ç¼“å­˜å‘½ä¸­: 100 tokens Ã— $0.075/1M = $0.0075
  - æ–° tokens: 50 tokens Ã— $0.15/1M = $0.0075
  - **æ€»æˆæœ¬é™ä½ 50%**

**è§¦å‘æ¡ä»¶**:
- System prompt ä¿æŒä¸å˜
- å†å²æ¶ˆæ¯éƒ¨åˆ†ç›¸åŒ
- 5 åˆ†é’Ÿå†…çš„è¯·æ±‚

**æŸ¥çœ‹ç¼“å­˜ç‡**:
```bash
python scripts/view_prompts.py --last 10
# è¾“å‡ºä¼šæ˜¾ç¤º: ç¼“å­˜: 100/150 (66.7%)
```

---

## ğŸš€ ä½¿ç”¨æŒ‡å—

### 1. å¦‚ä½•å¯ç”¨

**å¯¹äºæ–°ç”¨æˆ·**:
```sql
UPDATE users SET is_admin = true WHERE email = 'admin@example.com';
```

**å¯¹äºç°æœ‰ä¼šè¯**: åªè¦ç”¨æˆ·çš„ `is_admin=true`ï¼Œåç»­å¯¹è¯ä¼šè‡ªåŠ¨è®°å½•ã€‚

### 2. æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹ä»Šå¤©çš„æ‰€æœ‰ admin ç”¨æˆ·æ—¥å¿—
python scripts/view_prompts.py --today

# æŸ¥çœ‹æŸä¸ªç”¨æˆ·çš„æ—¥å¿—
python scripts/view_prompts.py --user-id 123

# æŸ¥çœ‹æŸä¸ªä¼šè¯çš„å®Œæ•´å¯¹è¯
python scripts/view_prompts.py --session-id abc-123 --show-full

# æŸ¥çœ‹æœ€è¿‘ 5 æ¡ï¼Œæ˜¾ç¤ºå®Œæ•´å†…å®¹
python scripts/view_prompts.py --last 5 --show-full

# å¯¼å‡ºä¸º JSON è¿›è¡Œåˆ†æ
python scripts/view_prompts.py --user-id 123 --export analysis.json
```

### 3. æ—¥å¿—å†…å®¹ç¤ºä¾‹

**è¯·æ±‚æ—¥å¿—**:
```json
{
  "timestamp": "2025-12-13T14:30:00",
  "user_id": 123,
  "session_id": "session-abc-123",
  "model": "gpt-4o-mini",
  "messages": [
    {
      "role": "system",
      "content": "ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„å¿ƒç†å’¨è¯¢å¸ˆ...\n\n## æ²»ç–—å¸ˆä¸ªæ€§åŒ–æŒ‡ä»¤\n\nXXæ²»ç–—å¸ˆçš„æŒ‡ä»¤...\n\n## å½“å‰ç”¨æˆ·æƒ…å†µ\n\nç”¨æˆ·ä¸Šä¸‹æ–‡..."
    },
    {
      "role": "user",
      "content": "æˆ‘æœ€è¿‘æ„Ÿè§‰å¾ˆç„¦è™‘..."
    }
  ],
  "request_params": {
    "temperature": 0.7,
    "max_tokens": 1000
  }
}
```

**å“åº”æ—¥å¿—**:
```json
{
  "timestamp": "2025-12-13T14:30:05",
  "user_id": 123,
  "session_id": "session-abc-123",
  "type": "response",
  "content": "æˆ‘èƒ½æ„Ÿå—åˆ°ä½ çš„ç„¦è™‘...",
  "usage": {
    "prompt_tokens": 1450,
    "completion_tokens": 180,
    "total_tokens": 1630,
    "prompt_tokens_details": {
      "cached_tokens": 1200
    }
  }
}
```

---

## ğŸ” æ•°æ®åˆ†æç¤ºä¾‹

### åˆ†æ Prompt æ•ˆæœ

```python
import json
from pathlib import Path

# åŠ è½½æ—¥å¿—
logs = []
for file in Path("logs/prompts").glob("*.jsonl"):
    with open(file) as f:
        logs.extend([json.loads(line) for line in f])

# ç»Ÿè®¡ç¼“å­˜ç‡
cache_rates = []
for log in logs:
    if log.get("type") == "response":
        usage = log.get("usage", {})
        details = usage.get("prompt_tokens_details", {})
        cached = details.get("cached_tokens", 0)
        total = usage.get("prompt_tokens", 1)
        cache_rates.append(cached / total * 100)

print(f"å¹³å‡ç¼“å­˜ç‡: {sum(cache_rates) / len(cache_rates):.1f}%")

# åˆ†æ prompt é•¿åº¦
prompt_lengths = []
for log in logs:
    if "messages" in log:
        system_msg = next((m for m in log["messages"] if m["role"] == "system"), None)
        if system_msg:
            prompt_lengths.append(len(system_msg["content"]))

print(f"å¹³å‡ Prompt é•¿åº¦: {sum(prompt_lengths) / len(prompt_lengths):.0f} å­—ç¬¦")
```

### å¯¹æ¯”ä¸åŒæ²»ç–—å¸ˆçš„æ•ˆæœ

```python
# æå–æ²»ç–—å¸ˆæŒ‡ä»¤éƒ¨åˆ†
therapist_prompts = {}
for log in logs:
    if "messages" in log:
        system_msg = next((m for m in log["messages"] if m["role"] == "system"), None)
        if system_msg and "## æ²»ç–—å¸ˆä¸ªæ€§åŒ–æŒ‡ä»¤" in system_msg["content"]:
            # æå–æ²»ç–—å¸ˆéƒ¨åˆ†
            content = system_msg["content"]
            start = content.find("## æ²»ç–—å¸ˆä¸ªæ€§åŒ–æŒ‡ä»¤")
            end = content.find("## å½“å‰ç”¨æˆ·æƒ…å†µ")
            therapist_section = content[start:end]

            # è®°å½•è¿™ä¸ªæ²»ç–—å¸ˆçš„ä½¿ç”¨æƒ…å†µ
            user_id = log["user_id"]
            if therapist_section not in therapist_prompts:
                therapist_prompts[therapist_section] = []
            therapist_prompts[therapist_section].append(log)

# è¾“å‡ºæ¯ä¸ªæ²»ç–—å¸ˆçš„ä½¿ç”¨ç»Ÿè®¡
for prompt, logs in therapist_prompts.items():
    print(f"\næ²»ç–—å¸ˆ Prompt (å‰50å­—): {prompt[:50]}...")
    print(f"  ä½¿ç”¨æ¬¡æ•°: {len(logs)}")
```

---

## ğŸ› ï¸ ç»´æŠ¤å’Œä¼˜åŒ–

### æ—¥å¿—æ¸…ç†

```bash
# å½’æ¡£ 30 å¤©å‰çš„æ—¥å¿—
mkdir -p logs/prompts/archive
find logs/prompts -name "*.jsonl" -mtime +30 -exec mv {} logs/prompts/archive/ \;

# å‹ç¼©å½’æ¡£
cd logs/prompts/archive
tar -czf prompts_2025-11.tar.gz therapist_prompts_2025-11-*.jsonl
rm therapist_prompts_2025-11-*.jsonl
```

### ç›‘æ§ç¼“å­˜æ•ˆç‡

æ·»åŠ åˆ° crontabï¼Œæ¯å¤©æ£€æŸ¥ç¼“å­˜ç‡ï¼š

```bash
# æ¯å¤© 23:00 æ£€æŸ¥ä»Šå¤©çš„ç¼“å­˜ç‡
0 23 * * * cd /path/to/backend && ./venv/bin/python scripts/analyze_cache.py --today
```

åˆ›å»º `scripts/analyze_cache.py`:

```python
#!/usr/bin/env python3
import json
from pathlib import Path
from datetime import datetime

log_file = Path(f"logs/prompts/therapist_prompts_{datetime.now().strftime('%Y-%m-%d')}.jsonl")

if not log_file.exists():
    print(f"No logs for today")
    exit(0)

cache_rates = []
with open(log_file) as f:
    for line in f:
        log = json.loads(line)
        if log.get("type") == "response":
            usage = log.get("usage", {})
            details = usage.get("prompt_tokens_details", {})
            cached = details.get("cached_tokens", 0)
            total = usage.get("prompt_tokens", 1)
            if total > 0:
                cache_rates.append(cached / total * 100)

if cache_rates:
    avg = sum(cache_rates) / len(cache_rates)
    print(f"Today's cache rate: {avg:.1f}% ({len(cache_rates)} requests)")
else:
    print("No cache data available")
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **éšç§ä¿æŠ¤**:
   - ä»… admin ç”¨æˆ·çš„å¯¹è¯ä¼šè¢«è®°å½•
   - æ—¥å¿—æ–‡ä»¶åŒ…å«æ•æ„Ÿå†…å®¹ï¼ŒåŠ¡å¿…ä¿æŠ¤å¥½è®¿é—®æƒé™
   - å·²åœ¨ `.gitignore` ä¸­æ’é™¤æ—¥å¿—æ–‡ä»¶

2. **ç£ç›˜ç©ºé—´**:
   - æ¯æ¡å®Œæ•´ prompt æ—¥å¿—çº¦ 2-5 KB
   - 100 æ¡å¯¹è¯çº¦ 500 KB
   - å»ºè®®å®šæœŸå½’æ¡£

3. **æ€§èƒ½å½±å“**:
   - HTTP hooks ä¼šç•¥å¾®å¢åŠ å»¶è¿Ÿï¼ˆ<10msï¼‰
   - ä½¿ç”¨ contextvarsï¼Œçº¿ç¨‹å®‰å…¨
   - å¯¹é admin ç”¨æˆ·æ— å½±å“

4. **è°ƒè¯•**:
   ```python
   # æŸ¥çœ‹æ—¥å¿—è®°å½•å™¨çŠ¶æ€
   from app.core.openai_logger import get_prompt_logger
   logger = get_prompt_logger()
   print(f"Log dir: {logger.log_dir}")
   print(f"Today's file: {logger._get_log_file_path()}")
   ```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference/chat)
- [Prompt Caching Guide](https://platform.openai.com/docs/guides/prompt-caching)
- [httpx Event Hooks](https://www.python-httpx.org/advanced/#event-hooks)

---

## ğŸ¤ è´¡çŒ®

å¦‚éœ€æ”¹è¿›æ—¥å¿—ç³»ç»Ÿï¼Œè¯·ï¼š

1. ä¿®æ”¹ `app/core/openai_logger.py` çš„è®°å½•é€»è¾‘
2. æ›´æ–° `scripts/view_prompts.py` å¢åŠ æ–°çš„æŸ¥çœ‹åŠŸèƒ½
3. è¡¥å……æœ¬æ–‡æ¡£
